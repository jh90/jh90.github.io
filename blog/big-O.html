<!DOCTYPE html>
<head>
  <title>Jake Hamilton's Î¦0 Technical Blog</title>
  <meta charset="UTF-8">
  <link rel="stylesheet" type="text/css" href="your-stylesheet-link-here.css">
</head>

<main>
  <h1>Big-O Notation: Algorithms and Time-Complexity</h1>
  <h2></h2>
  <h4>Mon Feb 1 2016</h4>

  <section>
    <p>
      Big-O notation is a form of asymptotic notation, meaning it expresses the limiting behavior of a function like an algorithm (program) as its argument (the number of inputs or input size) goes to infinity - in computer science, in terms of another simpler function like 2N+1 or N^3N. This allows us to see at-a-glance how our program's behavior will change as the input we give it linearly scales in size: some will also scale linearly (good) in the time or working space they require to execute the program or solve the problem, while others will scale geometrically/exponentially (bad) or even asymptotically (worse: this is the case for trying to break encryption like PGP, for example). Thus, two functions, algorithms, or programs may both be characterized by the notation O(n^2), which means their requirements to execute grow as the square of the size of the input, even though for each this may technically mean 'xn^2' for some differing constant coefficient x, and even though they may have very different absolute scales of expected input and runtime. In Big O notation we're looking at the <em>shape</em> of the curve that relates their input to requirements for output, rather than its 'size' with respect to particular axis-scales - which may, as implied already, not even be the same if we're looking at working space in one case and runtime in another.
  </section>
</main>